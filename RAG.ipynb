{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e772c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install -U torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7293a6",
   "metadata": {},
   "source": [
    "Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "ACCESS_TOKEN = os.getenv(\"ACCESS_TOKEN\") # reads .env file with ACCESS_TOKEN=<your hugging face access token>\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=ACCESS_TOKEN)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=quantization_config,\n",
    "                                             token=ACCESS_TOKEN)\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "            Context: {context}.\n",
    "            Question: {question}\"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        # { \"role\": \"model\", \"content\": \"Recurrent Attention (RAG)** is a novel neural network architecture specifically designed\" }\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer.encode(\n",
    "        formatted_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=250,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    response = response[len(formatted_prompt) :]  # remove input prompt from reponse\n",
    "    response = response.replace(\"<eos>\", \"\")  # remove eos token\n",
    "    return response\n",
    "\n",
    "\n",
    "question = \"What is a transformer?\"\n",
    "print(inference(question=question, context=\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7180ba",
   "metadata": {},
   "source": [
    "Document loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99dbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"/home/eversberg/Downloads/1706.03762.pdf\"),\n",
    "    PyPDFLoader(\"/home/eversberg/Downloads/2005.11401.pdf\"),\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275c4bf",
   "metadata": {},
   "source": [
    "Embeddings and vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ea525",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings\n",
    ")\n",
    "encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = encoder.embed_query(\"RAG\")\n",
    "embeddings2 = encoder.embed_query(docs[0].page_content)\n",
    "print(np.dot(embeddings1, embeddings2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c228fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ceaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "faiss_db = FAISS.from_documents(docs, encoder, distance_strategy=DistanceStrategy.DOT_PRODUCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4640da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a transformer?\"\n",
    "retrieved_docs = faiss_db.similarity_search(question, k=5)\n",
    "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(inference(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For this answer I used the following documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
